{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!ls"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This is an ugly workaround to allow reading of pickle files in pandas 0.19 (Misha's local version), while the files were produces in pandas 0.21 (on swan)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "import pandas.indexes \n",
    "sys.modules['pandas.core.indexes'] = pandas.indexes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def loadInputAsDF(fin_name):\n",
    "    #read original .npy files\n",
    "    if '.npy' in fin_name:\n",
    "        train_array = np.load(fin_name, encoding='bytes')\n",
    "        train_rec_array = train_array.view(np.recarray)\n",
    "        return pd.DataFrame.from_records(train_rec_array)\n",
    "    elif '.pickle' in fin_name:\n",
    "        return pd.read_pickle(fin_name)\n",
    "    else: \n",
    "        print(\"I do not know how to treat this input file: {}\".format(fin_name))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#train_file_name = 'train10000.npy'\n",
    "train_file_name = 'train_full_Nhardest5.pickle'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Read in the file properly for different file formats"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_df = loadInputAsDF(train_file_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_df.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_df.corr()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## EDA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#for var in ['recojet_pt', 'recojet_eta', 'recojet_phi', 'recojet_m',\n",
    "#       'recojet_sd_pt', 'recojet_sd_eta', 'recojet_sd_phi', 'recojet_sd_m',\n",
    "#       'n_constituents']:\n",
    "#    print(var)\n",
    "#    sns.jointplot(x='genjet_sd_m', y=var, data=train_df, kind='hex')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Feature engineering and drop some columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_df.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "columns_to_drop = []\n",
    "columns_arrays = ['constituents_pt', 'constituents_eta',\n",
    "       'constituents_phi', 'constituents_charge', 'constituents_dxy',\n",
    "       'constituents_dz', 'constituents_Eem', 'constituents_Ehad']\n",
    "columns_insignificant = ['recojet_eta', 'recojet_phi', \n",
    "       'recojet_sd_eta', 'recojet_sd_phi']\n",
    "columns_insignificant_const = ['constituents_eta_0',\n",
    "       'constituents_eta_1', 'constituents_eta_2', 'constituents_eta_3',\n",
    "       'constituents_eta_4', 'constituents_phi_0', 'constituents_phi_1',\n",
    "       'constituents_phi_2', 'constituents_phi_3', 'constituents_phi_4',\n",
    "       'constituents_charge_0', 'constituents_charge_1',\n",
    "       'constituents_charge_2', 'constituents_charge_3',\n",
    "       'constituents_charge_4']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#columns_to_drop.extend(columns_arrays)\n",
    "columns_to_drop.extend(columns_insignificant)\n",
    "columns_to_drop.extend(columns_insignificant_const)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#To be done only if those array columns have not been droped yet\n",
    "train_df.drop(columns_to_drop, axis=1, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_df.info()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Split and normalise"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, X_test, y_train, y_test = train_test_split(train_df.drop('genjet_sd_m', axis=1), train_df['genjet_sd_m'] , test_size=0.30, random_state=314)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# XGBoost regressor"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Convert to xgboost input structure"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import xgboost as xgb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# construct xgboost.DMatrix from numpy array, treat -999.0 as missing value\n",
    "xgbmat_train = xgb.DMatrix( data=X_train, label=y_train, missing = np.nan )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "isinstance(xgbmat_train , xgb.DMatrix)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Build the XGBoost model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate_loss(predictions, truth):  \n",
    "    #truth is xgb.DMatrix in fact, thust .get_label to get the y column\n",
    "    if isinstance(truth , xgb.DMatrix):\n",
    "        t = truth.get_label()\n",
    "    else:\n",
    "        t = truth\n",
    "    ratio = predictions / t\n",
    "    a = np.nanpercentile(ratio, 84, interpolation='nearest')  \n",
    "    b = np.nanpercentile(ratio, 16, interpolation='nearest')  \n",
    "    c = np.nanpercentile(ratio, 50, interpolation='nearest')  \n",
    "    loss = (a-b)/(2.*c)  \n",
    "    return loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate_loss_xgb(predictions, truth):  \n",
    "    loss = evaluate_loss(predictions, truth)\n",
    "    return ('xxx', loss)  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#preliminary parameters. will be fine-tuned in the GridSearch\n",
    "xgb_params = {'max_depth': 5, 'learning_rate':0.2, 'n_estimators':100,\n",
    "              'silent':1, 'random_state': 314, 'seed': 314, 'n_job':4}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "clf = xgb.XGBRegressor(**xgb_params)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Do a comparison of feature importance and extract the optimal number of trees"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#an adjusted function from this post: https://www.analyticsvidhya.com/blog/2016/03/complete-guide-parameter-tuning-xgboost-with-codes-python/\n",
    "def modelfit(alg, X_train, y_train, useTrainCV=True, cv_folds=5, early_stopping_rounds=50):\n",
    "    \n",
    "    if useTrainCV:\n",
    "        xgb_param = alg.get_xgb_params()\n",
    "        xgtrain = xgb.DMatrix(X_train, label=y_train)\n",
    "        cvresult = xgb.cv(xgb_param, xgtrain, num_boost_round=alg.get_params()['n_estimators'], nfold=cv_folds,\n",
    "            metrics='rmse', early_stopping_rounds=early_stopping_rounds, verbose_eval=True)\n",
    "        alg.set_params(n_estimators=cvresult.shape[0])\n",
    "        print(\"Decided on {} trees\".format(cvresult.shape[0]))\n",
    "\n",
    "    \n",
    "    #Fit the algorithm on the data\n",
    "    alg.fit(X_train, y_train, eval_metric=evaluate_loss_xgb)\n",
    "        \n",
    "    #Predict training set:\n",
    "    pred = alg.predict(X_train)\n",
    "        \n",
    "    from sklearn.metrics import mean_squared_error\n",
    "    from math import sqrt\n",
    "    #Print model report:\n",
    "    print(\"\\nModel Report\")\n",
    "    print(\"RMSE : %.4g\" % sqrt(mean_squared_error(y_train, pred)))\n",
    "    print(\"Custom loss : %.4g\" % evaluate_loss(y_train, pred))\n",
    "    \n",
    "    feat_imp = pd.Series(alg.get_booster().get_fscore()).sort_values(ascending=False)\n",
    "    feat_imp.plot(kind='bar', title='Feature Importances')\n",
    "    plt.ylabel('Feature Importance Score')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "modelfit(clf, X_train, y_train, early_stopping_rounds=10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## GridSearch to determine the optimal parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import GridSearchCV"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import make_scorer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The next cell is CPU intense! do not try it on the full dataset!!!!!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "param_test1 = {'max_depth': [3,5,7],\n",
    "               'min_child_weight': [1,3],\n",
    "               'gamma': [0,1e-3,1e-1],\n",
    "               'subsample': [0.6,0.8,1],\n",
    "               'colsample_bytree':[0.6,0.8,1],\n",
    "               'reg_alpha':[0, 1e-3, 1e-1],\n",
    "               'reg_lambda':[1, 1e-1, 1e-3]}\n",
    "gs1 = GridSearchCV(estimator=clf, param_grid=param_test1, \n",
    "                   scoring=make_scorer(evaluate_loss, greater_is_better=False),\n",
    "                   n_jobs=4, cv=5)\n",
    "gs1.fit(X_train, y_train)\n",
    "print(gs1.best_params_)\n",
    "print(gs1.best_score_)\n",
    "print(gs1.grid_scores_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "gs1.cv_results_"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Run XGBoost with the chosen optimal parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#preliminary parameters. will be fine-tuned in the GridSearch\n",
    "xgb_opt = {'learning_rate':0.1, 'n_estimators':50,\n",
    "           'silent':1, 'random_state': 314, 'seed': 314,\n",
    "           'colsample_bytree': 1, 'subsample': 1,\n",
    "           'gamma': 0, 'max_depth': 7, \n",
    "           'min_child_weight': 1, \n",
    "           'reg_alpha': 0.001, 'reg_lambda': 1}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "clf.set_params(**xgb_opt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "clf.fit(X_train, y_train, \n",
    "        eval_set=[(X_train, y_train), (X_test, y_test)],\n",
    "        eval_metric=evaluate_loss_xgb,\n",
    "        verbose=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "evals_result = clf.evals_result()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(10,6))\n",
    "plt.plot(range(clf.get_params()['n_estimators']), \n",
    "         evals_result['validation_0']['xxx'],\n",
    "         'b--', label='Train')\n",
    "plt.plot(range(clf.get_params()['n_estimators']), \n",
    "         evals_result['validation_1']['xxx'],\n",
    "         'r-', label='Test')\n",
    "plt.xlabel('N trees')\n",
    "plt.ylabel('Desired metrics')\n",
    "plt.legend()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
